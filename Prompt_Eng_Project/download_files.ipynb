{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pdfkit requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting frontend\n",
      "  Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting starlette>=0.12.0\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
      "Collecting uvicorn>=0.7.1\n",
      "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.4/62.4 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting aiofiles\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: itsdangerous>=1.1.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from frontend) (2.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from starlette>=0.12.0->frontend) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from starlette>=0.12.0->frontend) (4.3.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from uvicorn>=0.7.1->frontend) (8.0.4)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn>=0.7.1->frontend) (0.4.5)\n",
      "Installing collected packages: h11, aiofiles, uvicorn, starlette, frontend\n",
      "Successfully installed aiofiles-24.1.0 frontend-0.0.3 h11-0.14.0 starlette-0.37.2 uvicorn-0.30.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfkit in c:\\users\\chris\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chris\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chris\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\chris\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\chris\\anaconda3\\lib\\site-packages (1.24.7)\n",
      "Collecting fitz\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from PyPDF2) (4.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.6 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pymupdf) (1.24.6)\n",
      "Collecting configobj\n",
      "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
      "Collecting nipype\n",
      "  Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
      "     ---------------------------------------- 3.2/3.2 MB 9.7 MB/s eta 0:00:00\n",
      "Collecting httplib2\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.9/96.9 kB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\chris\\anaconda3\\lib\\site-packages (from fitz) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\chris\\anaconda3\\lib\\site-packages (from fitz) (1.4.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\chris\\anaconda3\\lib\\site-packages (from fitz) (1.9.1)\n",
      "Collecting nibabel\n",
      "  Downloading nibabel-5.2.1-py3-none-any.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 13.1 MB/s eta 0:00:00\n",
      "Collecting configparser\n",
      "  Downloading configparser-7.0.0-py3-none-any.whl (16 kB)\n",
      "Collecting pyxnat\n",
      "  Downloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 95.6/95.6 kB 5.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in c:\\users\\chris\\anaconda3\\lib\\site-packages (from configobj->fitz) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from httplib2->fitz) (3.0.9)\n",
      "Requirement already satisfied: packaging>=17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nibabel->fitz) (21.3)\n",
      "Requirement already satisfied: click>=6.6.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nipype->fitz) (8.0.4)\n",
      "Requirement already satisfied: filelock>=3.0.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nipype->fitz) (3.6.0)\n",
      "Collecting traits!=5.0,<6.4,>=4.6\n",
      "  Downloading traits-6.3.2-cp39-cp39-win_amd64.whl (5.0 MB)\n",
      "     ---------------------------------------- 5.0/5.0 MB 10.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nipype->fitz) (2.8.2)\n",
      "Collecting looseversion\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting simplejson>=3.8.0\n",
      "  Downloading simplejson-3.19.2-cp39-cp39-win_amd64.whl (75 kB)\n",
      "     ---------------------------------------- 75.5/75.5 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting rdflib>=5.0.0\n",
      "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
      "     ------------------------------------- 531.9/531.9 kB 11.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from nipype->fitz) (2.8.4)\n",
      "Collecting pydot>=1.2.3\n",
      "  Downloading pydot-2.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting prov>=1.5.2\n",
      "  Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
      "     ------------------------------------- 421.5/421.5 kB 13.3 MB/s eta 0:00:00\n",
      "Collecting etelemetry>=0.2.0\n",
      "  Downloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas->fitz) (2022.1)\n",
      "Requirement already satisfied: pathlib>=1.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from click>=6.6.0->nipype->fitz) (0.4.5)\n",
      "Collecting ci-info>=0.2\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting rdflib>=5.0.0\n",
      "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
      "     ------------------------------------- 528.1/528.1 kB 11.0 MB/s eta 0:00:00\n",
      "Collecting isodate<0.7.0,>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 41.7/41.7 kB ? eta 0:00:00\n",
      "Installing collected packages: looseversion, traits, simplejson, pydot, isodate, httplib2, configparser, configobj, ci-info, rdflib, pyxnat, nibabel, etelemetry, prov, nipype, fitz\n",
      "Successfully installed ci-info-0.3.0 configobj-5.0.8 configparser-7.0.0 etelemetry-0.3.1 fitz-0.0.1.dev2 httplib2-0.22.0 isodate-0.6.1 looseversion-1.3.0 nibabel-5.2.1 nipype-1.8.6 prov-2.0.1 pydot-2.0.0 pyxnat-1.6.2 rdflib-6.3.2 simplejson-3.19.2 traits-6.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfkit requests PyPDF2 beautifulsoup4 lxml pymupdf fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wkhtmltopdf\n",
      "  Downloading wkhtmltopdf-0.2.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wkhtmltopdf\n",
      "  Building wheel for wkhtmltopdf (setup.py): started\n",
      "  Building wheel for wkhtmltopdf (setup.py): finished with status 'done'\n",
      "  Created wheel for wkhtmltopdf: filename=wkhtmltopdf-0.2-py3-none-any.whl size=11133 sha256=0a4e5fcb15c789fefea97fe6961d9cb440f17e13ad974ad2ffe087169a7a59e4\n",
      "  Stored in directory: c:\\users\\chris\\appdata\\local\\pip\\cache\\wheels\\d9\\ac\\1d\\25b42a14d8813a8f1c2420ce552e84c547a28a9631dfdfad39\n",
      "Successfully built wkhtmltopdf\n",
      "Installing collected packages: wkhtmltopdf\n",
      "Successfully installed wkhtmltopdf-0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install wkhtmltopdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cloud.google.com_blog_transform_how-to-be-a-better.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfkit\n",
    "import requests\n",
    "from PyPDF2 import PdfReader, PdfMerger\n",
    "import re\n",
    "\n",
    "MAX_FILENAME_LENGTH = 50  # Maximum length for filenames\n",
    "\n",
    "def download_pdf(url, output_file):\n",
    "    \"\"\"\n",
    "    Download a PDF file from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): URL of the PDF file.\n",
    "    output_file (str): The output file path.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    with open(output_file, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "def is_pdf(url):\n",
    "    \"\"\"\n",
    "    Check if the given URL points to a PDF.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): URL to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the URL points to a PDF, False otherwise.\n",
    "    \"\"\"\n",
    "    response = requests.head(url, allow_redirects=True)\n",
    "    return 'application/pdf' in response.headers.get('Content-Type', '')\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"\n",
    "    Sanitize a filename by removing invalid characters and truncating it to a maximum length.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The filename to sanitize.\n",
    "\n",
    "    Returns:\n",
    "    str: The sanitized filename.\n",
    "    \"\"\"\n",
    "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', \"\", filename.strip())\n",
    "    return sanitized[:MAX_FILENAME_LENGTH]\n",
    "\n",
    "def get_filename_from_url(url):\n",
    "    \"\"\"\n",
    "    Generate a filename from the URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to generate a filename from.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated filename.\n",
    "    \"\"\"\n",
    "    return sanitize_filename(url.split('//')[-1].replace('/', '_'))\n",
    "\n",
    "def get_title_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract the first line or title from a PDF file.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted title or a sanitized filename part as a fallback.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    if len(reader.pages) > 0:\n",
    "        first_page = reader.pages[0]\n",
    "        text = first_page.extract_text().split('\\n')[0]\n",
    "        truncated_text = text[:MAX_FILENAME_LENGTH].strip()\n",
    "        return sanitize_filename(truncated_text)\n",
    "    return os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "def generate_unique_filename(filename, existing_files):\n",
    "    \"\"\"\n",
    "    Generate a unique filename by appending a number if the filename already exists.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The desired filename.\n",
    "    existing_files (set): A set of filenames that already exist.\n",
    "\n",
    "    Returns:\n",
    "    str: A unique filename.\n",
    "    \"\"\"\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    counter = 1\n",
    "    unique_filename = filename\n",
    "    while unique_filename in existing_files:\n",
    "        unique_filename = f\"{base}_{counter}{ext}\"\n",
    "        counter += 1\n",
    "    existing_files.add(unique_filename)\n",
    "    return unique_filename\n",
    "\n",
    "def urls_to_pdf(urls, output_file):\n",
    "    \"\"\"\n",
    "    Convert a list of URLs to a single PDF file. Handles both direct PDF links and webpages.\n",
    "\n",
    "    Parameters:\n",
    "    urls (list): List of URLs to be converted.\n",
    "    output_file (str): The output PDF file name.\n",
    "    \"\"\"\n",
    "    pdf_files = []\n",
    "    existing_files = set()\n",
    "    path_to_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'  # Use the correct path\n",
    "    config = pdfkit.configuration(wkhtmltopdf=path_to_wkhtmltopdf)\n",
    "\n",
    "    options = {\n",
    "        'quiet': '',\n",
    "        'no-stop-slow-scripts': '',\n",
    "        'disable-smart-shrinking': '',\n",
    "        'javascript-delay': '5000'  # Increase delay to 5 seconds\n",
    "    }\n",
    "\n",
    "    for url in urls:\n",
    "        if is_pdf(url):\n",
    "            # Website provided is PDF\n",
    "            temp_pdf = \"temp.pdf\"\n",
    "            download_pdf(url, temp_pdf)\n",
    "            title = get_title_from_pdf(temp_pdf)\n",
    "            pdf_output = f\"{title}.pdf\"\n",
    "            pdf_output = generate_unique_filename(pdf_output, existing_files)\n",
    "            try:\n",
    "                os.rename(temp_pdf, pdf_output)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            except OSError as e:\n",
    "                print(f\"OSError renaming {temp_pdf} to {pdf_output}: {e}\")\n",
    "            pdf_files.append(pdf_output)\n",
    "        else:\n",
    "            # Website provided was URL\n",
    "            temp_pdf = \"output.pdf\"\n",
    "\n",
    "            # Generate PDF\n",
    "            pdfkit.from_url(url, temp_pdf, configuration=config, options=options)\n",
    "\n",
    "            # Get unique name\n",
    "            filename = get_filename_from_url(url)\n",
    "            pdf_output = f\"{filename}.pdf\"\n",
    "            \n",
    "            # Check duplications\n",
    "            pdf_output = generate_unique_filename(pdf_output, existing_files)\n",
    "\n",
    "            # Rename file:\n",
    "            try:\n",
    "                os.rename(temp_pdf, pdf_output)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            except OSError as e:\n",
    "                print(f\"OSError renaming {temp_pdf} to {pdf_output}: {e}\")\n",
    "\n",
    "            # Track List:\n",
    "            pdf_files.append(pdf_output)\n",
    "\n",
    "        print(pdf_files)\n",
    "\n",
    "    # # Merge all PDFs into a single PDF\n",
    "    # merger = PdfMerger()\n",
    "\n",
    "    # for pdf in pdf_files:\n",
    "    #     merger.append(pdf)\n",
    "\n",
    "    # merger.write(output_file)  # Save the merged PDF with the specified output filename\n",
    "    # merger.close()\n",
    "\n",
    "    # # Clean up individual PDFs\n",
    "    # for pdf in pdf_files:\n",
    "    #     try:\n",
    "    #         os.remove(pdf)\n",
    "    #     except FileNotFoundError:\n",
    "    #         pass\n",
    "    #     except OSError as e:\n",
    "    #         print(f\"OSError removing {pdf}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "urls = ['https://cloud.google.com/blog/transform/how-to-be-a-better-prompt-engineer',\n",
    "        # 'https://arxiv.org/pdf/2311.05661',\n",
    "        # 'https://arxiv.org/pdf/2311.16452',\n",
    "        # 'https://arxiv.org/abs/2306.12509',\n",
    "        # 'https://arxiv.org/pdf/2401.14423',\n",
    "]\n",
    "\n",
    "output_file = 'output.pdf'\n",
    "urls_to_pdf(urls, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocks of Code not required:\n",
    "    # # Merge all PDFs into a single PDF\n",
    "    # merger = PdfMerger()\n",
    "\n",
    "    # for pdf in pdf_files:\n",
    "    #     merger.append(pdf)\n",
    "\n",
    "    # merger.write(output_file)  # Save the merged PDF with the specified output filename\n",
    "    # merger.close()\n",
    "\n",
    "    # # Clean up individual PDFs\n",
    "    # for pdf in pdf_files:\n",
    "    #     os.remove(pdf)\n",
    "\n",
    "    # Debugging information\n",
    "    # print(f\"Converting URL to PDF: {url}\")\n",
    "    # print(f\"Saving as: {pdf_output}\")\n",
    "\n",
    "            # # Rename file. Ignore the filenotfounderror, since the renaming still happens.\n",
    "            # try:\n",
    "            #     os.rename(\"output.pdf\", pdf_output)\n",
    "            # except Exception:\n",
    "            #     pass\n",
    "\n",
    "# Example usage\n",
    "# urls = ['https://digitalcommons.chapman.edu/cgi/viewcontent.cgi?article=1044&context=librarian_articles',\n",
    "# 'https://arxiv.org/pdf/2311.05232',\n",
    "# 'https://arxiv.org/pdf/2312.16171v1',\n",
    "# 'https://arxiv.org/pdf/2310.05029',\n",
    "# 'https://arxiv.org/pdf/2310.01714',\n",
    "# 'https://arxiv.org/pdf/2309.11495',\n",
    "# 'https://arxiv.org/pdf/2309.04269',\n",
    "# 'https://arxiv.org/pdf/2310.08395',\n",
    "# 'https://arxiv.org/pdf/2310.08123',\n",
    "# 'https://arxiv.org/pdf/2310.08101',\n",
    "# 'https://arxiv.org/pdf/2310.00297'\n",
    "# ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2022.10.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
